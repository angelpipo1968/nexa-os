version: '3.8'

services:
  # 1. Reverse Proxy & Web Server
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./build:/usr/share/nginx/html # Serves the React Frontend (Run 'npm run build' first)
    depends_on:
      - token-api
      - ollama

  # 2. Token Manager & Proxy Service
  token-api:
    build: ./token-manager
    ports:
      - "8087:8087" # Exposed for dev mode compatibility
    environment:
      - REDIS_URL=redis://redis:6379
      - PORT=8087
      - LOCAL_AI_URL=http://ollama:11434/v1/chat/completions
      # Pass through API Keys from host .env or define here
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GOOGLE_AI_API_KEY=${GOOGLE_AI_API_KEY}
      - QWEN_API_KEY=${QWEN_API_KEY}
    env_file:
      - .env.local
    depends_on:
      - redis
      - ollama

  # 3. Local AI Engine (Ollama)
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434" # Exposed for local tools/CLI access
    # To use GPU, uncomment below (requires NVIDIA Container Toolkit)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # 4. Usage Database
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  ollama_data:
  redis_data:
